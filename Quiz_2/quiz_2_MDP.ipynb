{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8a3c62-216c-446d-851d-7249cc67787a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {align:left;display:block} \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {align:left;display:block} \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e27c8c-7f53-4e9f-b4da-7e0343fb6313",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDP)\n",
    "----\n",
    "\n",
    "**Value Iteration Process with Policy Changes in MDP**\n",
    "\n",
    "We begin with a Markov Decision Process (MDP) where an agent decides whether to invest conservatively (C) or aggressively (A) in a financial portfolio. The objective is to find an optimal policy maximizing long-term rewards.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Defining the MDP Components**\n",
    "\n",
    "**States (S):**\n",
    "\n",
    "- Low Wealth (L)\n",
    "- Medium Wealth (M)\n",
    "- High Wealth (H)\n",
    "\n",
    "**Actions (A):**\n",
    "\n",
    "- Conservative (C)\n",
    "- Aggressive (A)\n",
    "\n",
    "**Transition Probabilities:**\n",
    "\n",
    ">| Current State | Action | Next State Probabilities     |\n",
    "| ------------- | ------ | ---------------------------- |\n",
    "| Low (L)       | C      | 80% Stay in L, 20% Move to M |\n",
    "| Low (L)       | A      | 60% Stay in L, 40% Move to M |\n",
    "| Medium (M)    | C      | 70% Stay in M, 30% Move to H |\n",
    "| Medium (M)    | A      | 50% Stay in M, 50% Move to H |\n",
    "| High (H)      | C      | 90% Stay in H, 10% Drop to M |\n",
    "| High (H)      | A      | 70% Stay in H, 30% Drop to M |\n",
    "\n",
    "**Rewards:**\n",
    "\n",
    "- Low Wealth (L): -1\n",
    "- Medium Wealth (M): 3\n",
    "- High Wealth (H): 5\n",
    "\n",
    "**Discount Factor (γ):** 0.9\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6becd-538b-4010-b4ed-c2e03c6c2e1a",
   "metadata": {},
   "source": [
    "### **Step 2: Value Iteration Updates**\n",
    "\n",
    "We initialize values: $V_0(L) = 0$, $V_0(M) = 0$, $V_0(H) = 0$.\n",
    "\n",
    "#### **Iteration 1**\n",
    "\n",
    "Using Bellman’s equation:\n",
    "\n",
    ">$\n",
    "V_1(s) = \\max_a \\left[ R(s) + \\gamma \\sum_{s'} P(s' | s, a) V_0(s') \\right]\n",
    "$\n",
    "\n",
    "For **Low Wealth (L):**\n",
    "\n",
    ">$\n",
    "V_1(L) = \\max \\left[ -1 + 0.9(0.8V_0(L) + 0.2V_0(M)), -1 + 0.9(0.6V_0(L) + 0.4V_0(M)) \\right]\n",
    "$\n",
    "\n",
    "For **Medium Wealth (M):**\n",
    "\n",
    ">$\n",
    "V_1(M) = \\max \\left[ 3 + 0.9(0.7V_0(M) + 0.3V_0(H)), 3 + 0.9(0.5V_0(M) + 0.5V_0(H)) \\right]\n",
    "$\n",
    "\n",
    "For **High Wealth (H):**\n",
    "\n",
    ">$\n",
    "V_1(H) = \\max \\left[ 5 + 0.9(0.9V_0(H) + 0.1V_0(M)), 5 + 0.9(0.7V_0(H) + 0.3V_0(M)) \\right]\n",
    "$\n",
    "\n",
    "Since $V_0(L) = V_0(M) = V_0(H) = 0$, the initial values are just the rewards.\n",
    "\n",
    ">$\n",
    "V_1(L) = -1, \\quad V_1(M) = 3, \\quad V_1(H) = 5\n",
    "$\n",
    "\n",
    "#### **Policy Evaluation after Iteration 1**\n",
    "\n",
    "> \\$\n",
    "Q(L, C) = -1 + 0.9(0.8(-1) + 0.2(3)) = -1.44 (should be -1.18)\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(L, A) = -1 + 0.9(0.6(-1) + 0.4(3)) = -0.76 (should be -0.46)\n",
    "\\$\n",
    "\n",
    "Complete the rest...\n",
    "\n",
    "**Policy at Iteration 1:**\n",
    "- L → Aggressive (A)\n",
    "- M → Aggressive (A)\n",
    "- H → Conservative (C)\n",
    "\n",
    "\n",
    "#### **Iteration 2**\n",
    "\n",
    "Updating $V_2(s)$:\n",
    "\n",
    ">$\n",
    "V_2(L) = \\max \\left[ -1 + 0.9(0.8(-1) + 0.2(3)), -1 + 0.9(0.6(-1) + 0.4(3)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_2(M) = \\max \\left[ 3 + 0.9(0.7(3) + 0.3(5)), 3 + 0.9(0.5(3) + 0.5(5)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_2(H) = \\max \\left[ 5 + 0.9(0.9(5) + 0.1(3)), 5 + 0.9(0.7(5) + 0.3(3)) \\right]\n",
    "$\n",
    "\n",
    "Computing the above:\n",
    "\n",
    ">$\n",
    "V_2(L) = -0.46, \\quad V_2(M) = 6.6, \\quad V_2(H) = 9.32\n",
    "$\n",
    "\n",
    "\n",
    "#### **Policy Evaluation after Iteration 2**\n",
    "\n",
    "Complete the iteration...\n",
    "\n",
    "> \\$\n",
    "Q(L, C) = -1 + 0.9(0.8(-0.46) + 0.2(6.6)) = -0.1432\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(L, A) = -1 + 0.9(0.6(-0.46) + 0.4(6.6)) = 1.1276\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(M, C) = 3 + 0.9(0.7(6.6) + 0.3(9.32)) = 9.6744\n",
    "\\$\n",
    "\n",
    ">\\$\n",
    "Q(M, A) = 3 + 0.9(0.5(6.6) + 0.5(9.32)) = 10.164\n",
    "\\$\n",
    "\n",
    ">\\$\n",
    "Q(H, C) = 5 + 0.9(0.9(9.32) + 0.1(6.6)) = 13.1432\n",
    "\\$\n",
    "\n",
    ">\\$\n",
    "Q(H, A) = 5 + 0.9(0.7(9.32) + 0.3(6.6)) = 12.6536\n",
    "\\$ \n",
    "\n",
    "\n",
    "\n",
    "**Policy at Iteration 2:**\n",
    "- L → Aggresive (A)\n",
    "- M → Aggresive (A)\n",
    "- H → Conservative (C)\n",
    "\n",
    "#### **Iteration 3**\n",
    "\n",
    "Updating $V_3(s)$:\n",
    "\n",
    "TODO:\n",
    "\n",
    ">$\n",
    "V_3(L) = \\max \\left[ -1 + 0.9(0.8(-0.46) + 0.2(6.6)), -1 + 0.9(0.6(-0.46) + 0.4(6.6)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_3(M) = \\max \\left[ 3 + 0.9(0.7(6.6) + 0.3(9.32)), 3 + 0.9(0.5(6.6) + 0.5(9.32)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_3(H) = \\max \\left[ 5 + 0.9(0.9(9.32) + 0.1(6.6)), 5 + 0.9(0.7(9.32) + 0.3(6.6)) \\right]\n",
    "$\n",
    "\n",
    "Computing the above:\n",
    "\n",
    ">$\n",
    "V_3(L) = 1.12, \\quad V_3(M) = 10.16, \\quad V_3(H) = 13.14\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "#### **Policy Change Analysis**\n",
    "\n",
    "From **Iteration 2 to Iteration 3**, let’s check the action values to determine if the policy changed.\n",
    "\n",
    "For **Low Wealth (L):**\n",
    "\n",
    ">$\n",
    "Q(L, C) = -1 + 0.9(0.8(1.12) + 0.2(10.16)) = 1.62$\n",
    "\n",
    ">$\n",
    "Q(L, A) = -1 + 0.9(0.6(1.12) + 0.2(10.16)) = 1.43$\n",
    "\n",
    "For **Medium Wealth (M):**\n",
    "\n",
    ">$\n",
    "Q(M, C) = 3 + 0.9(0.7(10.16) + 0.3(13.14)) = 12.94\n",
    "$\n",
    "\n",
    ">$\n",
    "Q(M, A) = 3 + 0.9(0.5(10.16) + 0.5(13.14)) = 13.48\n",
    "$\n",
    "\n",
    "For **High Wealth (H):**\n",
    "\n",
    ">$\n",
    "Q(H, C) = 5 + 0.9(0.9(13.14) + 0.1(10.16)) = 16.54\n",
    "$\n",
    "\n",
    ">$\n",
    "Q(H, A) = 5 + 0.9(0.7(13.14) + 0.3(10.16)) = 16.007\n",
    "$\n",
    "\n",
    "Compare $Q(L, A), Q(L, C)$ and $Q(H, C),  Q(H, A)$, decide the policy updates:\n",
    "\n",
    "\n",
    "1. As Q(L,C)>Q(L,A), the optimal action for Low Wealth (L) is Conservative (C).\n",
    "\n",
    "- Policy Update: Switch to Conservative (C) if it was previously Aggressive (A).\n",
    "\n",
    "\n",
    "\n",
    "2. As Q(H,C)>Q(H,A),  the optimal action for Low Wealth (L) is Conservative (C).\n",
    "\n",
    "- Policy Update: Switch to Conservative (C) if it was previously Aggressive (A).\n",
    "\n",
    "- **Low Wealth (L)** → Conservative (C)\n",
    "- **Medium Wealth (M)** → Aggresive (A)\n",
    "- **High Wealth (H)** → Conservative (C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f040f7b-f23f-466a-bdb1-07ea5b715899",
   "metadata": {},
   "source": [
    "### Summary: Policy Evolution Over Iterations\n",
    "\n",
    ">| State  | Iteration 1 | Iteration 2 | Iteration 3 |\r\n",
    "|--------|------------|------------|------------|\r\n",
    "| Low      | Aggressive (A) | Aggressive (A)| Conservative (C)|\r\n",
    "| Medium   | Aggressive (A) | Aggressive (A)| Aggressive (A) |\r\n",
    "| High     | Conservative (C)| Conservative (C)| Conservative (C)|\n",
    "\n",
    "Low Wealth (L)\n",
    "\n",
    "Iteration 1 & 2: Go Aggressive (A)\n",
    "Iteration 3: Switch to Conservative (C)\n",
    "Analysis: At first, being aggressive is better because it gives a higher expected reward based on the probabilities and future rewards. But by the third iteration, things stabilize, and the model shows that being conservative is better in the long run, probably because it avoids the risk of staying in the low-wealth state.\n",
    "\n",
    "Medium Wealth (M)\n",
    "\n",
    "All Iterations: Stay Aggressive (A)\n",
    "Analysis: For medium wealth, being aggressive always gives a higher value in all iterations. This means taking risks at this level pays off more in the long run compared to being conservative, likely because there's a higher chance of moving to the high-wealth state.\n",
    "\n",
    "High Wealth (H)\n",
    "\n",
    "All Iterations: Stay Conservative (C)\n",
    "Analysis: Once you're in the high-wealth state, being conservative is the best choice in all iterations. This is because it maximizes stability and reduces the risk of dropping to medium wealth. Being aggressive has a higher chance of moving down, which lowers the long-term expected reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d932a85-7a0d-4cf2-9657-072b9c7809d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
